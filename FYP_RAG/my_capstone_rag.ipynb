{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3f55b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\FYP\\FYP_RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# RAG System Tools and IBM Cloud Connectors\n",
    "import os\n",
    "from docling.document_converter import DocumentConverter\n",
    "from langchain_docling import DoclingLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter # FIX 1 (Package install: langchain-text-splitters)\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_classic.chains import RetrievalQA # FIX 2 (Package install: langchain-classic)\n",
    "from langchain_ibm import WatsonxLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df40969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Credentials and URL loaded into session memory (Correct variable names used).\n"
     ]
    }
   ],
   "source": [
    "# IBM watsonx.ai Credentials and Connection Setup\n",
    "import os\n",
    "\n",
    "os.environ[\"WATSONX_API_KEY\"] = \"heqOEuyYXy-ngbBIHva-GR8-0HNBFystyN9V6Vv2oMJB\"\n",
    "os.environ[\"IBM_PROJECT_ID\"] = \"0ed5949a-bd21-4f66-9733-4646506adc34\"\n",
    "os.environ[\"WATSONX_URL\"] = \"https://us-south.ml.cloud.ibm.com\" \n",
    "\n",
    "print(\"✅ Credentials and URL loaded into session memory (Correct variable names used).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61d35eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\FYP\\FYP_RAG\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f\"Current Working Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47d0e8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load all PDFs from directory: c:\\FYP\\FYP_RAG\\granite-snack-cookbook\\fyp_document\n",
      "✅ Loaded 130 total pages/documents across all files.\n",
      "✅ Split documents into 551 chunks of size 1000.\n"
     ]
    }
   ],
   "source": [
    "# RAG Data Ingestion and Preparation Pipeline\n",
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. Define the correct directory path\n",
    "directory_path = os.path.join(os.getcwd(), \"granite-snack-cookbook\", \"fyp_document\")\n",
    "\n",
    "print(f\"Attempting to load all PDFs from directory: {directory_path}\")\n",
    "\n",
    "# 2. Load Documents using DirectoryLoader with PyPDFLoader\n",
    "# PyPDFLoader is now correctly imported and used.\n",
    "loader = DirectoryLoader(\n",
    "    path=directory_path,\n",
    "    glob=\"*.pdf\",  # Only load files ending in .pdf\n",
    "    loader_cls=PyPDFLoader  \n",
    ")\n",
    "\n",
    "# This should execute and load the files quickly.\n",
    "all_documents = loader.load()\n",
    "\n",
    "print(f\"✅ Loaded {len(all_documents)} total pages/documents across all files.\")\n",
    "\n",
    "# 3. Split Text into Chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=100 \n",
    ")\n",
    "texts = text_splitter.split_documents(all_documents)\n",
    "print(f\"✅ Split documents into {len(texts)} chunks of size 1000.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83867eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23008574\\AppData\\Local\\Temp\\ipykernel_9604\\2549352001.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "C:\\Users\\23008574\\AppData\\Local\\Temp\\ipykernel_9604\\2549352001.py:7: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "2025-11-25 21:14:15,146 - INFO - Use pytorch device_name: cpu\n",
      "2025-11-25 21:14:15,147 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2025-11-25 21:17:37,996 - INFO - Loading faiss with AVX2 support.\n",
      "2025-11-25 21:17:38,079 - INFO - Successfully loaded faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector store created successfully using FAISS.\n"
     ]
    }
   ],
   "source": [
    "# Vector Indexing and Storage\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS \n",
    "\n",
    "# 1. Create Embeddings\n",
    "# This uses a pre-trained model to convert text chunks into numerical vectors.\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "# 2. Create Vector Store (FAISS Index)\n",
    "# This stores the vectors for fast retrieval.\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "print(\"✅ Vector store created successfully using FAISS.\")\n",
    "\n",
    "# OPTIONAL: Save the index for faster reloading later\n",
    "db.save_local(\"faiss_index_who_report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8ed1eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 21:20:10,218 - INFO - Client successfully initialized\n",
      "2025-11-25 21:20:12,359 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-10-29&project_id=0ed5949a-bd21-4f66-9733-4646506adc34&filters=%21lifecycle_withdrawn&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-11-25 21:20:12,638 - INFO - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-10-29&project_id=0ed5949a-bd21-4f66-9733-4646506adc34&filters=%21lifecycle_withdrawn&limit=200'\n",
      "c:\\FYP\\FYP_RAG\\.venv\\Lib\\site-packages\\ibm_watsonx_ai\\foundation_models\\utils\\utils.py:427: LifecycleWarning: Model 'ibm/granite-3-8b-instruct' is in deprecated state from 2025-11-24 until 2026-02-22. IDs of alternative models: ibm/granite-4-h-small. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
      "  warn(model_state_warning, category=LifecycleWarning)\n",
      "2025-11-25 21:20:57,014 - WARNING - Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "2025-11-25 21:21:18,550 - WARNING - Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "2025-11-25 21:21:18,552 - WARNING - Cannot set gray stroke color because /'P1' is an invalid float value\n",
      "2025-11-25 21:21:30,058 - INFO - Use pytorch device_name: cpu\n",
      "2025-11-25 21:21:30,059 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OPTIMIZED RAG Setup Complete.\n"
     ]
    }
   ],
   "source": [
    "# The Highly Optimized RAG Application Setup\n",
    "import os \n",
    "from langchain_ibm import WatsonxLLM \n",
    "from langchain_classic.chains import RetrievalQA\n",
    "\n",
    "# Corrected Imports for RAG components\n",
    "from langchain_community.document_loaders import DirectoryLoader, PDFPlumberLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# --- 1. LLM Initialization (Reliability & Output Length) ---\n",
    "watsonx_llm = WatsonxLLM(                  \n",
    "    model_id=\"ibm/granite-3-8b-instruct\",                   \n",
    "    project_id=os.getenv(\"IBM_PROJECT_ID\"),\n",
    "    # FIX: Use 'params' dictionary for generation controls (required by your LangChain-IBM version)\n",
    "    params={'max_new_tokens': 1024, 'temperature': 0.1} # Lower temperature for factual accuracy\n",
    ")\n",
    "\n",
    "# --- 2. Document Processing and Vector Store Creation (Accuracy Tuning) ---\n",
    "loader = DirectoryLoader(\n",
    "    path='./granite-snack-cookbook/fyp_document', \n",
    "    loader_cls=PDFPlumberLoader, # ⬅️ USE PDFPlumberLoader HERE\n",
    "    glob='*.pdf'             \n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "# OPTIMIZATION: Adjusted Chunking for better metadata retrieval (like ISBN/DOI)\n",
    "text_splitter = RecursiveCharacterTextSplitter( \n",
    "    chunk_size=300,     # Very small chunk size\n",
    "    chunk_overlap=150   # Greatly increased overlap to capture context across breaks\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\" # Explicitly named embedding model\n",
    ")\n",
    "\n",
    "db = FAISS.from_documents(docs, embeddings) \n",
    "\n",
    "# --- 3. Create the RetrievalQA Chain (Citation & Speed) ---\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=watsonx_llm,           \n",
    "    chain_type=\"stuff\",      \n",
    "    # OPTIMIZATION: Set k=3 for faster response time and less noise\n",
    "    retriever=db.as_retriever(search_kwargs={'k': 10}), \n",
    "    # CRITICAL FOR CITATION: Returns the document chunks that were used by the LLM\n",
    "    return_source_documents=True \n",
    ")     \n",
    "\n",
    "print(\"✅ OPTIMIZED RAG Setup Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b65ce43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retrieved 5 documents.\n",
      "\n",
      "--- CONTENT OF RETRIEVED CHUNKS ---\n",
      "\n",
      "--- Document 1 (Page 8) ---\n",
      "Snippet: Development Goal (SDG) targets by 2030. This is evidenced by the falling annual rate of reduction in indicators such as the maternal mortality ratio, under-five and neonatal mortality rates, premature...\n",
      "\n",
      "--- Document 2 (Page 11) ---\n",
      "Snippet: Reductions in both maternal and child mortality were among the targets of the Millennium Development Goals (MDGs), declared in 2000, that the world strived to achieve by 2015. They continue to be amon...\n",
      "\n",
      "--- Document 3 (Page 5) ---\n",
      "Snippet: risks to health, particularly for the most vulnerable and societies. populations. For the world to attain the targets of the Sustainable Development Goals (SDGs) by 2030, a substantial increase in foc...\n",
      "\n",
      "--- Document 4 (Page 11) ---\n",
      "Snippet: child mortality Improvement of maternal and child health has been high on the global development agenda since the turn of the millennium. Reductions in both maternal and child mortality were among the...\n",
      "\n",
      "--- Document 5 (Page 12) ---\n",
      "Snippet: the SDG era as the ARR of the global maternal mortality between 2016 and 2020 after having decreased during the ratio plummeted to -0.04% (UI: -1.6–1.1%) between 2016 MDG era. However, the levels of t...\n"
     ]
    }
   ],
   "source": [
    "# 1. Manually retrieve the top 5 most similar documents\n",
    "retrieved_docs = db.similarity_search(question, k=5)\n",
    "\n",
    "print(f\"✅ Retrieved {len(retrieved_docs)} documents.\")\n",
    "print(\"\\n--- CONTENT OF RETRIEVED CHUNKS ---\")\n",
    "\n",
    "# 2. Print the content and page number of each retrieved document\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    # This prints the first 200 characters of the chunk\n",
    "    content_snippet = doc.page_content[:200].replace('\\n', ' ') \n",
    "    page_number = doc.metadata.get('page', 'N/A')\n",
    "    \n",
    "    # CRITICAL CHECK: Look for the ISBN, DOI, or 'PAPERBACK' in the output below.\n",
    "    print(f\"\\n--- Document {i+1} (Page {page_number}) ---\")\n",
    "    print(f\"Snippet: {content_snippet}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5de6e098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 21:25:25,185 - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-10-29 \"HTTP/1.1 200 OK\"\n",
      "2025-11-25 21:25:25,190 - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-10-29'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the Sustainable Development Goal (SDG) target for the global maternal mortality ratio by 2030?\n",
      "---\n",
      "Answer: \n",
      "\n",
      "The Sustainable Development Goal (SDG) target for the global maternal mortality ratio by 2030 is to reduce it to less than 70 maternal deaths per 100,000 live births. This represents a 2.7% (UI: 2.0--3.2%) average annual rate of reduction (ARR).\n",
      "\n",
      "--- Sources Used (for Citation) ---\n",
      "File: granite-snack-cookbook\\fyp_document\\world-health-statistics-2023_20230519_.pdf (Page: 8)\n",
      "File: granite-snack-cookbook\\fyp_document\\world-health-statistics-2023_20230519_.pdf (Page: 11)\n",
      "File: granite-snack-cookbook\\fyp_document\\world-health-statistics-2023_20230519_.pdf (Page: 5)\n",
      "File: granite-snack-cookbook\\fyp_document\\world-health-statistics-2023_20230519_.pdf (Page: 11)\n",
      "File: granite-snack-cookbook\\fyp_document\\world-health-statistics-2023_20230519_.pdf (Page: 12)\n",
      "File: granite-snack-cookbook\\fyp_document\\world-health-statistics-2023_20230519_.pdf (Page: 8)\n",
      "File: granite-snack-cookbook\\fyp_document\\world-health-statistics-2023_20230519_.pdf (Page: 12)\n",
      "File: granite-snack-cookbook\\fyp_document\\world-health-statistics-2023_20230519_.pdf (Page: 11)\n",
      "File: granite-snack-cookbook\\fyp_document\\world-health-statistics-2023_20230519_.pdf (Page: 17)\n",
      "File: granite-snack-cookbook\\fyp_document\\world-health-statistics-2023_20230519_.pdf (Page: 13)\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the Sustainable Development Goal (SDG) target for the global maternal mortality ratio by 2030?\"\n",
    "\n",
    "# ⬇️ FIX: Use .invoke() instead of .run() ⬇️\n",
    "result_dict = qa_chain.invoke({'query': question})\n",
    "\n",
    "# --- Extract the Answer ---\n",
    "answer = result_dict['result']\n",
    "\n",
    "# --- Extract the Source Documents for Citation ---\n",
    "sources = result_dict['source_documents']\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(\"---\")\n",
    "print(f\"Answer: {answer}\")\n",
    "\n",
    "# --- CITATION SECTION (meets your criteria) ---\n",
    "print(\"\\n--- Sources Used (for Citation) ---\")\n",
    "for doc in sources:\n",
    "    # This extracts the source file name and page number from the document metadata\n",
    "    source_file = doc.metadata.get('source', 'N/A').split('/')[-1]\n",
    "    page_number = doc.metadata.get('page', 'N/A')\n",
    "    print(f\"File: {source_file} (Page: {page_number})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
